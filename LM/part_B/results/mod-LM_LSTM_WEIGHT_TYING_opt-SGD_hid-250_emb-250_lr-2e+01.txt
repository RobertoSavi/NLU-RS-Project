[Model: LM_LSTM_WEIGHT_TYING, Optimizer: SGD, Hidden-size: 250, Embedding-size: 250, Learning-rate: 20]
[Epoch: 1, PPL: 438.6764, Best PPL: 438.6764]
[Epoch: 2, PPL: 351.4592, Best PPL: 351.4592]
[Epoch: 3, PPL: 332.8818, Best PPL: 332.8818]
[Epoch: 4, PPL: 303.7380, Best PPL: 303.7380]
[Epoch: 5, PPL: 300.0925, Best PPL: 300.0925]
[Epoch: 6, PPL: 286.3731, Best PPL: 286.3731]
[Epoch: 7, PPL: 275.0315, Best PPL: 275.0315]
[Epoch: 8, PPL: 272.3302, Best PPL: 272.3302]
[Epoch: 9, PPL: 263.7524, Best PPL: 263.7524]
[Epoch: 10, PPL: 262.1820, Best PPL: 262.1820]
[Epoch: 11, PPL: 257.3415, Best PPL: 257.3415]
[Epoch: 12, PPL: 259.5559, Best PPL: 257.3415]
[Epoch: 13, PPL: 252.0059, Best PPL: 252.0059]
[Epoch: 14, PPL: 247.0172, Best PPL: 247.0172]
[Epoch: 15, PPL: 248.8683, Best PPL: 247.0172]
[Epoch: 16, PPL: 249.1478, Best PPL: 247.0172]
[Epoch: 17, PPL: 248.4369, Best PPL: 247.0172]
[Epoch: 18, PPL: 243.4722, Best PPL: 243.4722]
[Epoch: 19, PPL: 245.5194, Best PPL: 243.4722]
[Epoch: 20, PPL: 247.7318, Best PPL: 243.4722]
[Epoch: 21, PPL: 243.7753, Best PPL: 243.4722]
[Epoch: 22, PPL: 243.5787, Best PPL: 243.4722]
[Epoch: 23, PPL: 243.0612, Best PPL: 243.0612]
[Epoch: 24, PPL: 245.0322, Best PPL: 243.0612]
[Epoch: 25, PPL: 244.2792, Best PPL: 243.0612]
[Epoch: 26, PPL: 248.6929, Best PPL: 243.0612]
[Epoch: 27, PPL: 243.8666, Best PPL: 243.0612]
[Epoch: 28, PPL: 245.1588, Best PPL: 243.0612]
[Final PPL: 222.4369]
