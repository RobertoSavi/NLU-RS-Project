\documentclass[a4paper]{article}

\usepackage{INTERSPEECH2021}

% Put the lab number of the corresponding exercise
\title{NLU course project 1}
\name{Roberto Savi (257858)}

\address{
  University of Trento}
\email{roberto.savi@studenti.unitn.it}

\begin{document}

\maketitle

Dear students, \\
here you can find a complete description of the sections that you need to write for the mini-report. You have to write a mini-report of \textbf{max 1 page (references, tables and images are excluded from the count)} for each last exercise of labs 4 (LM) and 5 (NLU). \textbf{Reports longer than 1 page will not be checked.} The purpose of this is to give you a way to report cleanly the results and give you space to describe what you have done and/or the originality that you have added to the exercise.
\\
\textbf{If you did part A only, you have to just report the results in a table with a small description.}

\section{Introduction (approx. 100 words)}
\begin{itemize}
    \item \textit{I first let the training perform with the stadanrd parameters provided in the lab, saving the results in standard_lr0_001.pt}
    \item \textit{Then I increased the learning rate to 0.05, saving the results in standard_lr0_05.pt}
    \item \textit{Then, I increased the batch size of training from 64 to 128 and the batch size of development and test from 128 to 256, saving the results in double_batch_size.pt. This has been done to save time at the expenses of more memory, however the time spent for training was actually higher than expected so this change was reverted.}
    \item \textit{Then, I tried different learning rates, starting from 0.0001 to 5, to see which was giving the best results without touching the rest of the hyperparameters. The model was saved in best_lr_model.pt.}
    \item \textit{Based on this results, i individuated the best learning rate in 1.0, i also noticed how after a threshold increasing the learning rate also meant that the model finished training before the 100 epochs, meaning that the model achivied the best results possible with those parameters. So i used 1.0 as learning rate and tried a last tuning on the hidden_size and embedding_size parameters to see which configuration was yielding the best results. I tried various combinations from emb_size=50 and hid_size=100 to emb_size=300 and hid_size=200. 
    What i noticed was that out of all the models only the last was training at all with the large learning rate, and the training lasted for just 18 epochs before stopping. So i tried reducing the learning rate to 0.5 to see what happened. Here again only the last set of hyperparams allowed to properly train the model with very similar results, so i tried one last time with a learning rate of 0.05.}
    \item \textit{Then again the only model training was the last one, so i tried switching hidden and embedding size configurations to more passable ones, keeping the learning rate at 0.05.}
    \item \textit{Seeing the results of the experiments i decided to opt for a network with a learning rate of 0.1 on which to apply the successive modifications.}
\end{itemize}
\section{Implementation details (max approx. 200-300 words)}
Do not explain the backbone deep neural network (e.g. RNN or BERT). Instead, focus on what you did on top of it. \textbf{Add references if you take inspiration from the code of others}

\section{Results}
Add tables and explain how you evaluated your model. Tables and images of plots or confusion matrices do not count in the page limit.


\bibliographystyle{IEEEtran}

\bibliography{mybib}


\end{document}
